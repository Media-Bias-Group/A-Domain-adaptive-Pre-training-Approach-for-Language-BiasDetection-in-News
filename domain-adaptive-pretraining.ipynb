{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This script applies Domain-adaptive pretraining to BERT,RoBERTa,BART,and T5. The final pre-trained models can be found at:\n\nRequired data to run this script:\n- the WNC corpus: https://github.com/rpryzant/neutralizing-bias","metadata":{"id":"UN5U7W0b7X-b","papermill":{"duration":0.015104,"end_time":"2021-09-24T12:57:54.41903","exception":false,"start_time":"2021-09-24T12:57:54.403926","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install transformers\n!pip install openpyxl\n!pip install sentencepiece\nimport time\nimport openpyxl\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport io\nimport random\nimport sys\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score,f1_score,precision_score,recall_score,accuracy_score\nimport transformers\nimport sentencepiece\nfrom transformers import T5Tokenizer,T5EncoderModel,AdamW,BertModel,BertTokenizer,RobertaModel,RobertaTokenizer,BartModel,BartTokenizer\nfrom torch.utils.data import DataLoader,TensorDataset,ConcatDataset,RandomSampler","metadata":{"id":"-EW98CVr7bMX","outputId":"2b007272-ebe2-41e3-e7b6-17fefffdf702","papermill":{"duration":21.776564,"end_time":"2021-09-24T12:58:16.209532","exception":false,"start_time":"2021-09-24T12:57:54.432968","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function split train dataset into train, validation and test sets\ndef train_test (text,labels,test_size):\n  train_text, test_text, train_labels, test_labels = train_test_split(text, \n                                                                    labels, \n                                                                    random_state=2018, \n                                                                    test_size=test_size,\n                                                                    stratify=labels)\n  return train_text, test_text, train_labels, test_labels","metadata":{"id":"lMQo98GZfSlQ","papermill":{"duration":0.026304,"end_time":"2021-09-24T12:58:16.325607","exception":false,"start_time":"2021-09-24T12:58:16.299303","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#function to tokenize sentences. Respective model must be uncommented\n#tokenizer = T5Tokenizer.from_pretrained('t5-base')\n#tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n#tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef tokenize(sentences,labels,max_length = None):\n  \"tokenizes input and returns tokenized input + labels as tensors\"\n\n  input_ids = []\n  attention_masks = []\n\n  for text in sentences.to_list():\n      encodings = tokenizer.encode_plus(text,add_special_tokens = True,max_length = max_length\n                                        ,truncation = True, padding = 'max_length',return_attention_mask = True)\n      input_ids.append(encodings['input_ids'])\n      attention_masks.append(encodings['attention_mask'])\n\n  return torch.tensor(input_ids),torch.tensor(attention_masks),torch.tensor(labels.to_list())","metadata":{"id":"kY5iqRlwfYd1","papermill":{"duration":7.772323,"end_time":"2021-09-24T12:58:24.116674","exception":false,"start_time":"2021-09-24T12:58:16.344351","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to get predictions for test data\ndef predict(model,dataloader):\n\n  predictions = []\n  for batch in dataloader:\n    batch = [r.to(device) for r in batch]\n    sent_id, mask, labels = batch\n    with torch.no_grad():\n      output = model(sent_id, attention_mask=mask,labels = labels)\n      preds = output[1]\n      preds = preds.detach().cpu().numpy()\n      predictions.append(np.argmax(preds, axis = 1).flatten())\n\n  #merge sublists of predictions\n  predictions = [label for batch in predictions for label in batch]\n\n  return predictions","metadata":{"id":"tYLxdLbofq1A","papermill":{"duration":0.030543,"end_time":"2021-09-24T12:58:24.171021","exception":false,"start_time":"2021-09-24T12:58:24.140478","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set seed\nnp.random.seed(0)\ntorch.manual_seed(0)   \nrandom.seed(0)    \ntorch.cuda.manual_seed_all(0)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","metadata":{"id":"b8wGoA61M4gn","papermill":{"duration":0.03261,"end_time":"2021-09-24T12:58:24.225481","exception":false,"start_time":"2021-09-24T12:58:24.192871","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#read WNC corpus \ndf_wiki = pd.read_excel('WNC.xlsx')\ndf_wiki.dropna(inplace=True)","metadata":{"id":"kYHCogOqdfvC","papermill":{"duration":0.027235,"end_time":"2021-09-24T12:58:24.274741","exception":false,"start_time":"2021-09-24T12:58:24.247506","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train test split + tokenization\ntrain_text, test_text, train_labels, test_labels = train_test(df_wiki['text'], df_wiki['label_bias'],0.2)\ntrain_input_ids,train_attention_masks,train_y = tokenize(train_text, train_labels)\ntest_input_ids,test_attention_masks,test_y = tokenize(test_text,test_labels) \ntrain_data_wiki = TensorDataset(train_input_ids, train_attention_masks, train_y)\ntest_data_wiki = TensorDataset(test_input_ids, test_attention_masks, test_y)","metadata":{"id":"xkb84DERme9r","papermill":{"duration":0.027531,"end_time":"2021-09-24T12:58:24.324983","exception":false,"start_time":"2021-09-24T12:58:24.297452","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define dataloader and epochs\nepochs = 1\nbatch_size = 32\n\ntrain_sampler = RandomSampler(train_data_wiki)\ntest_sampler = RandomSampler(test_data_wiki)\n\ntrain_dataloader = DataLoader(train_data_wiki,sampler= train_sampler, batch_size=batch_size)\ntest_dataloader = DataLoader(test_data_wiki,sampler= test_sampler, batch_size=batch_size)","metadata":{"id":"C3aj7vxNJTiM","papermill":{"duration":0.03044,"end_time":"2021-09-24T12:58:40.324228","exception":false,"start_time":"2021-09-24T12:58:40.293788","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define loss\ncross_entropy = nn.CrossEntropyLoss()","metadata":{"id":"X_9S6NvBe-fO","papermill":{"duration":0.027027,"end_time":"2021-09-24T12:58:40.371438","exception":false,"start_time":"2021-09-24T12:58:40.344411","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create model:RoBERTa\n\n# class RobertaClass(torch.nn.Module):\n#     def __init__(self):\n#         super(RobertaClass, self).__init__()\n#         self.roberta = RobertaModel#.from_pretrained(\"roberta-base\")\n#         self.vocab_transform = torch.nn.Linear(768, 768)\n#         self.dropout = torch.nn.Dropout(0.2)\n#         self.classifier1 = nn.Linear(768,2)\n\n#     def forward(self, input_ids, attention_mask,labels):\n#         output_1 = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n#         hidden_state = output_1[0]\n#         pooler = hidden_state[:, 0]\n#         pooler = self.vocab_transform(pooler)\n#         pooler = self.dropout(pooler)\n#         output = self.classifier1(pooler)\n#         loss = cross_entropy(output,labels)\n\n#         return loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create model: BART\n\n# class BartClass(torch.nn.Module):\n#     def __init__(self):\n#         super(BartClass, self).__init__()\n#         self.bart = BartModel.from_pretrained(\"facebook/bart-base\")\n#         self.vocab_transform = torch.nn.Linear(768, 768)\n#         self.dropout = torch.nn.Dropout(0.2)\n#         self.classifier1 = nn.Linear(768,2)\n\n#     def forward(self, input_ids, attention_mask,labels):\n#         output_1 = self.bart(input_ids=input_ids, attention_mask=attention_mask)\n#         hidden_state = output_1[0]\n#         pooler = hidden_state[:, 0]\n#         pooler = self.vocab_transform(pooler)\n#         pooler = self.dropout(pooler)\n#         output = self.classifier1(pooler)\n#         loss = cross_entropy(output,labels)\n\n#         return loss","metadata":{"id":"Y9qI1SEdbHMt","papermill":{"duration":0.029042,"end_time":"2021-09-24T12:58:40.42673","exception":false,"start_time":"2021-09-24T12:58:40.397688","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create model: Bert\n\n# class BertClass(torch.nn.Module):\n#     def __init__(self):\n#         super(BertClass, self).__init__()\n#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n#         self.vocab_transform = torch.nn.Linear(768, 768)\n#         self.dropout = torch.nn.Dropout(0.1)\n#         self.classifier1 = nn.Linear(768,2)\n\n#     def forward(self, input_ids, attention_mask,labels):\n#         output_1 = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n#         hidden_state = output_1[0]\n#         pooler = hidden_state[:, 0]\n#         pooler = self.vocab_transform(pooler)\n#         pooler = self.dropout(pooler)\n#         output = self.classifier1(pooler)\n#         loss = cross_entropy(output,labels)\n\n#         return loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create model: T5\n\n# class T5Class(torch.nn.Module):\n#     def __init__(self):\n#         super(T5Class, self).__init__()\n#         self.T5 = T5EncoderModel.from_pretrained(\"t5-base\")\n#         self.vocab_transform = torch.nn.Linear(768, 768)\n#         self.dropout = torch.nn.Dropout(0.1)\n#         self.classifier1 = nn.Linear(768,2)\n\n#     def forward(self, input_ids, attention_mask,labels):\n#         output_1 = self.T5(input_ids=input_ids, attention_mask=attention_mask)\n#         hidden_state = output_1[0]\n#         pooler = hidden_state[:, 0]\n#         pooler = self.vocab_transform(pooler)\n#         pooler = self.dropout(pooler)\n#         output = self.classifier1(pooler)\n#         loss = cross_entropy(output,labels)\n\n#         return loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#connect to GPU\nif torch.cuda.is_available():       \n    device = torch.device(\"cuda:0\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('Device name:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"id":"i2BwHSj6cDri","outputId":"aa93a2de-5060-47cc-c733-af3bcb442f12","papermill":{"duration":0.074651,"end_time":"2021-09-24T12:58:40.521086","exception":false,"start_time":"2021-09-24T12:58:40.446435","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#instantiate model: uncomment model you want to train\n\n# model = BertClass()\n# model = RobertaClass()\n# model = BartClass()\n# model = T5Class()\n\nmodel = model.to(device)\noptim = AdamW(model.parameters(), lr=1e-5)","metadata":{"papermill":{"duration":29.992468,"end_time":"2021-09-24T12:59:10.534451","exception":false,"start_time":"2021-09-24T12:58:40.541983","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train function\ndef train(dataloader):\n\n  model.train()\n  total_loss = 0\n  counter = 0\n    \n  for index,batch in enumerate(dataloader):\n    counter += 1\n    sys.stdout.write('\\r Batch {}/{}'.format(counter,len(dataloader)))\n    optim.zero_grad()\n    batch = [r.to(device) for r in batch]\n    sent_id, mask, labels = batch\n    loss = model(sent_id, attention_mask=mask,labels = labels)\n    loss.backward() \n    total_loss = total_loss+loss.item()\n    optim.step()\n    del batch,sent_id,mask,labels\n        \n  avg_loss = total_loss / len(dataloader)\n  return avg_loss","metadata":{"id":"o79JiLrW5MfB","papermill":{"duration":0.031646,"end_time":"2021-09-24T12:59:10.587149","exception":false,"start_time":"2021-09-24T12:59:10.555503","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test function\n\ndef validate(dataloader):\n    model.eval()\n    total_loss = 0\n    print(\"\\nValidating...\")\n    counter = 0\n    for batch in dataloader:\n      counter +=1\n      batch = [r.to(device) for r in batch]\n      sent_id, mask, labels = batch\n\n      with torch.no_grad():\n        loss = model(sent_id, attention_mask=mask,labels = labels)\n        total_loss = total_loss+loss\n\n    avg_loss = total_loss / len(dataloader) \n    return avg_loss","metadata":{"id":"_GYQcOX_fhjv","papermill":{"duration":0.028306,"end_time":"2021-09-24T12:59:10.637067","exception":false,"start_time":"2021-09-24T12:59:10.608761","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train/validate function\n\ndef train_validate(train_dataloader,test_dataloader):\n  best_valid_loss = float('inf')\n\n  # empty lists to store training and validation loss of each epoch\n  train_losses=[]\n  valid_losses=[]\n\n  #for each epoch\n  for epoch in range(epochs):\n        \n    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n    \n    #train model\n    train_loss = train(train_dataloader)\n    if torch.cuda.is_available():\n      torch.cuda.empty_cache()\n    \n    #evaluate model\n    valid_loss = validate(test_dataloader)\n    \n    #save the best model\n    if valid_loss < best_valid_loss:\n      best_valid_loss = valid_loss\n      torch.save(model.state_dict(), 'pytorch_model.bin') #insert path here\n      \n    #if validation loss increases, stop training\n    elif valid_loss >= best_valid_loss:\n      print(\"\\n Validation loss not decreased, Model of previous epoch saved\")\n      break\n    \n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'Validation Loss: {valid_loss:.3f}')","metadata":{"id":"Est1Bn9UAb3w","papermill":{"duration":0.030173,"end_time":"2021-09-24T12:59:10.687414","exception":false,"start_time":"2021-09-24T12:59:10.657241","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#apply training and validation\ntrain_validate(train_dataloader,test_dataloader)","metadata":{"id":"YTg22-3fbSI-","outputId":"8398d94c-19ef-44c3-88d3-74c26987f02e","papermill":{"duration":0.025912,"end_time":"2021-09-24T12:59:10.733686","exception":false,"start_time":"2021-09-24T12:59:10.707774","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}