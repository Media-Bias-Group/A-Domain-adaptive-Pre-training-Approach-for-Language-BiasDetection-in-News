# A-Domain-adaptive-Pre-training-Approach-for-Language-Bias-Detection-in-News
The repository contains the data files and scripts corresponding to the paper "A Domain-adaptive Pre-training Approach for Language Bias Detection in News".

The models can be found anonymously on: https://drive.google.com/drive/u/4/folders/1-A1hGKeu-27X9I4ySkja5vMlVscnF8GR
- "DA-Roberta.bin": Domain-adaptive Pre-training with RoBERTa.
- "DA-T5.bin": Domain-adaptive Pre-training with T5.
- "DA-BERT.bin": Domain-adaptive Pre-training with BERT.
- "DA-BART.bin": Domain-adaptive Pre-training with BART.
- "classifier.weights.pt" and "classifier.bias.pt": Parameters for classification layer + bias used for all models prior to Domain-adaptive Pre-training to achieve maximum comparability between approaches.

# Description of files
