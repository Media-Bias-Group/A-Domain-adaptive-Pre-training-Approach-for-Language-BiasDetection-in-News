{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This script fine-tunes and evaluates DA-RoBERTa, DA-BERT, DA-BART, and DA-T5 on the BABE dataset by 5-fold cross-validation:\n\nRequired data to run this script:\n- BABE.xlsx\n- the pretrained model that should be evaluated (selected model from ) ","metadata":{"id":"sWQHUy3sA2Ir"}},{"cell_type":"code","source":"!pip install transformers\n!pip install openpyxl\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport json\nimport io\nimport sys\nimport random\nimport openpyxl\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\nfrom sklearn.metrics import roc_auc_score,f1_score,precision_score,recall_score,accuracy_score,confusion_matrix\nimport transformers\nfrom transformers import AdamW,BertTokenizer,BertModel,RobertaTokenizer,RobertaModel,T5EncoderModel,T5Tokenizer,BartModel,BartTokenizer\nfrom torch.utils.data import DataLoader,TensorDataset,RandomSampler","metadata":{"id":"2FbVfplXA-Nf","outputId":"b45a868e-b678-47f5-94ff-3063b37893c7","execution":{"iopub.status.busy":"2022-01-16T13:49:47.391177Z","iopub.execute_input":"2022-01-16T13:49:47.391551Z","iopub.status.idle":"2022-01-16T13:50:12.887214Z","shell.execute_reply.started":"2022-01-16T13:49:47.391462Z","shell.execute_reply":"2022-01-16T13:50:12.886381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create model architecture** (Uncomment respective model which should be evaluated)","metadata":{"id":"6VWxBejP6T3E"}},{"cell_type":"markdown","source":"**RoBERTa**","metadata":{}},{"cell_type":"code","source":"# class RobertaClass(torch.nn.Module):\n#     def __init__(self):\n#         super(RobertaClass, self).__init__()\n#         self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n#         self.vocab_transform = torch.nn.Linear(768, 768)\n#         self.dropout = torch.nn.Dropout(0.2)\n#         self.classifier1 = torch.nn.Linear(768,2)\n\n#     def forward(self, input_ids, attention_mask):\n#         output_1 = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n#         hidden_state = output_1[0]\n#         pooler = hidden_state[:, 0]\n#         pooler = self.vocab_transform(pooler)\n#         pooler = self.dropout(pooler)\n#         output = self.classifier1(pooler)\n\n#         return output","metadata":{"id":"k5OstJYpMw5I","execution":{"iopub.status.busy":"2022-01-08T13:31:39.865633Z","iopub.execute_input":"2022-01-08T13:31:39.86612Z","iopub.status.idle":"2022-01-08T13:31:39.879666Z","shell.execute_reply.started":"2022-01-08T13:31:39.866083Z","shell.execute_reply":"2022-01-08T13:31:39.879023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**BERT**","metadata":{}},{"cell_type":"code","source":"# class BertClass(torch.nn.Module):\n#     def __init__(self):\n#         super(BertClass, self).__init__()\n#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n#         self.vocab_transform = torch.nn.Linear(768, 768)\n#         self.dropout = torch.nn.Dropout(0.2)\n#         self.classifier1 = torch.nn.Linear(768,2)\n\n#     def forward(self, input_ids, attention_mask):\n#         output_1 = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n#         hidden_state = output_1[0]\n#         pooler = hidden_state[:, 0]\n#         pooler = self.vocab_transform(pooler)\n#         pooler = self.dropout(pooler)\n#         output = self.classifier1(pooler)\n\n#         return output","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:42:01.075779Z","iopub.execute_input":"2022-01-08T13:42:01.076405Z","iopub.status.idle":"2022-01-08T13:42:01.083666Z","shell.execute_reply.started":"2022-01-08T13:42:01.076367Z","shell.execute_reply":"2022-01-08T13:42:01.082799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**T5**","metadata":{}},{"cell_type":"code","source":"#create model\n# class T5Class(torch.nn.Module):\n#     def __init__(self):\n#         super(T5Class, self).__init__()\n#         self.T5 = T5EncoderModel.from_pretrained(\"t5-base\")\n#         self.vocab_transform = torch.nn.Linear(768, 768)\n#         self.dropout = torch.nn.Dropout(0.2)\n#         self.classifier1 = nn.Linear(768,2)\n\n#     def forward(self, input_ids, attention_mask):\n#         output_1 = self.T5(input_ids=input_ids, attention_mask=attention_mask)\n#         hidden_state = output_1[0]\n#         pooler = hidden_state[:, 0]\n#         pooler = self.vocab_transform(pooler)\n#         pooler = self.dropout(pooler)\n#         output = self.classifier1(pooler)\n\n#         return output","metadata":{"execution":{"iopub.status.busy":"2022-01-16T13:50:31.703549Z","iopub.execute_input":"2022-01-16T13:50:31.704177Z","iopub.status.idle":"2022-01-16T13:50:31.712003Z","shell.execute_reply.started":"2022-01-16T13:50:31.704118Z","shell.execute_reply":"2022-01-16T13:50:31.711017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**BART**","metadata":{}},{"cell_type":"code","source":"# #create model\n# class BartClass(torch.nn.Module):\n#     def __init__(self):\n#         super(BartClass, self).__init__()\n#         self.bart = BartModel.from_pretrained(\"facebook/bart-base\")\n#         self.vocab_transform = torch.nn.Linear(768, 768)\n#         self.dropout = torch.nn.Dropout(0.2)\n#         self.classifier1 = nn.Linear(768,2)\n\n#     def forward(self, input_ids, attention_mask):\n#         output_1 = self.bart(input_ids=input_ids, attention_mask=attention_mask)\n#         hidden_state = output_1[0]\n#         pooler = hidden_state[:, 0]\n#         pooler = self.vocab_transform(pooler)\n#         pooler = self.dropout(pooler)\n#         output = self.classifier1(pooler)\n\n#         return output","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:59:31.92387Z","iopub.execute_input":"2022-01-08T13:59:31.924316Z","iopub.status.idle":"2022-01-08T13:59:31.931859Z","shell.execute_reply.started":"2022-01-08T13:59:31.924281Z","shell.execute_reply":"2022-01-08T13:59:31.930848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Connect to GPU**","metadata":{"id":"uNRsqqn26YOu"}},{"cell_type":"code","source":"if torch.cuda.is_available():       \n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('Device name:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"id":"CVAo1cx4J7RT","outputId":"8a700457-85c8-4ba2-e1f1-fb2e72b3f277","execution":{"iopub.status.busy":"2022-01-16T13:50:34.736381Z","iopub.execute_input":"2022-01-16T13:50:34.736954Z","iopub.status.idle":"2022-01-16T13:50:34.790225Z","shell.execute_reply.started":"2022-01-16T13:50:34.736913Z","shell.execute_reply":"2022-01-16T13:50:34.789204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load pre-trained domain-adapted weights/parameters for the model:** You might have to adapt the path pointing to the domain-adapted model","metadata":{"id":"dC7ABPwV6jH5"}},{"cell_type":"code","source":"#load weights of pretrained news model\n#weight_dict = torch.load('Roberta.bin')\n#weight_dict = torch.load('BERT.bin')\n#weight_dict = torch.load('T5.bin')\n#weight_dict = torch.load('BART.bin')\n\n#load saved classifier weights + classifier bias --> we use same parameters for the final classification of all models to achieve maximum comparability\nclassifier_weights = torch.load('../input/domainadaptivepretrainingjcdl/classifier.weights.pt')\nclassifier_bias = torch.load('../input/domainadaptivepretrainingjcdl/classifier.bias.pt')\n\n#insert weights and bias into weight dict\nweight_dict['classifier1.weight'] = classifier_weights\nweight_dict['classifier1.bias'] = classifier_bias","metadata":{"id":"C7YkR_bhyd6P","execution":{"iopub.status.busy":"2022-01-16T13:51:00.784267Z","iopub.execute_input":"2022-01-16T13:51:00.784923Z","iopub.status.idle":"2022-01-16T13:51:11.557579Z","shell.execute_reply.started":"2022-01-16T13:51:00.784886Z","shell.execute_reply":"2022-01-16T13:51:11.55684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load BABE Data:** You might have to adapt the path again","metadata":{"id":"WktGY1UUmztM"}},{"cell_type":"code","source":"df = pd.read_excel(\"BABE.xlsx\")\ndf = df[df['label_bias']!= 'No agreement']\ndf['Label_bias_0-1'] = df['label_bias'].map({'Biased':1,'Non-biased':0})\ndf.head(3)","metadata":{"id":"Aq7SkwILbrO9","outputId":"36c41f76-6aa1-4fb3-a9b5-f29875ad2194","execution":{"iopub.status.busy":"2022-01-16T13:51:40.708695Z","iopub.execute_input":"2022-01-16T13:51:40.708992Z","iopub.status.idle":"2022-01-16T13:51:41.747378Z","shell.execute_reply.started":"2022-01-16T13:51:40.708959Z","shell.execute_reply":"2022-01-16T13:51:41.746663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Define Cross-Validation,Tokenizer,Batch Size,Epochs,Loss, and Seeds**","metadata":{"id":"HqbHOVdl7aJh"}},{"cell_type":"code","source":"np.random.seed(2018)\ntorch.manual_seed(2018)   \nrandom.seed(2018)    \ntorch.cuda.manual_seed_all(2018)\nrandom.seed(2018)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nkfold = StratifiedKFold(n_splits = 3,shuffle = True,random_state=2)\n#tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n#tokenizer = T5Tokenizer.from_pretrained('t5-base')\n#tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\ncross_entropy = nn.CrossEntropyLoss()\n\nepochs = 10 #we implement an early stopping criterion. Fine-tuning is actually not done for 10 epochs\nbatch_size = 32","metadata":{"id":"NOkpsCuGuQK3","outputId":"626c1b1c-54ef-4248-a80f-26c03c502456","execution":{"iopub.status.busy":"2022-01-16T14:16:47.625891Z","iopub.execute_input":"2022-01-16T14:16:47.626163Z","iopub.status.idle":"2022-01-16T14:16:50.905503Z","shell.execute_reply.started":"2022-01-16T14:16:47.626114Z","shell.execute_reply":"2022-01-16T14:16:50.904624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Define functions for fine-tuning and validation**","metadata":{"id":"GUoIFq7n7ekg"}},{"cell_type":"code","source":"def train(model):\n\n    model.train()\n    total_loss = 0\n\n    for batch in train_dataloader:\n        optim_dbert.zero_grad()\n        batch = [r.to(device) for r in batch]\n        sent_id, mask, labels = batch\n        outputs = model(sent_id, attention_mask=mask)\n        loss = cross_entropy(outputs,labels)\n        total_loss = total_loss+loss.item()\n        loss.backward()\n        optim_dbert.step()\n\n    avg_loss = total_loss / len(train_dataloader)\n\n    return avg_loss","metadata":{"id":"oA6Np8VLxbuF","execution":{"iopub.status.busy":"2022-01-16T14:16:50.907102Z","iopub.execute_input":"2022-01-16T14:16:50.907443Z","iopub.status.idle":"2022-01-16T14:16:50.91461Z","shell.execute_reply.started":"2022-01-16T14:16:50.907399Z","shell.execute_reply":"2022-01-16T14:16:50.913794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(model):\n\n    model.eval()\n    total_loss = 0\n\n    print(\"\\n   Validating...\")\n\n    for batch in test_dataloader:\n        batch = [r.to(device) for r in batch]\n        sent_id, mask, labels = batch\n\n        with torch.no_grad():\n            outputs = model(sent_id, attention_mask=mask)\n            loss = cross_entropy(outputs,labels)\n            total_loss = total_loss+loss.item()\n\n    avg_loss = total_loss / len(test_dataloader) \n\n    return avg_loss","metadata":{"id":"7VvtdBe3xj0h","execution":{"iopub.status.busy":"2022-01-16T14:16:50.916209Z","iopub.execute_input":"2022-01-16T14:16:50.916723Z","iopub.status.idle":"2022-01-16T14:16:50.929015Z","shell.execute_reply.started":"2022-01-16T14:16:50.916682Z","shell.execute_reply":"2022-01-16T14:16:50.928196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#combine train and validate function: get train and validation loss for every cross-validation split, save best-performing model, and get predictions on the held-out test set to calculate evaluation metrics\n\ndef train_validate_pred(model):\n    best_valid_loss = float('inf')\n\n    # empty lists to store training and validation loss of each epoch\n    train_losses=[]\n    valid_losses=[]\n\n    #for each epoch\n    for epoch in range(epochs):\n\n        print('\\n   Epoch {} / {}'.format(epoch+1,epochs))\n\n        #train model\n        train_loss = train(model)\n\n        #evaluate model\n        valid_loss = validate(model)\n\n        #save the best model\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            global model_dbert\n            torch.save(model.state_dict(), 'saved_weights.pt')\n\n\n        #if validation loss increases, stop training\n        elif valid_loss >= best_valid_loss:\n            print(\"\\n Validation loss not decreased, Model of previous epoch saved\")\n            break\n\n        print(f'\\n    Training Loss: {train_loss:.3f}')\n        print(f'    Validation Loss: {valid_loss:.3f}')\n  \n    #predict\n    path = 'saved_weights.pt'\n    model.load_state_dict(torch.load(path))\n    with torch.no_grad():\n        preds = model(test_seq.to(device), test_mask.to(device))\n        preds = preds.detach().cpu().numpy()\n    preds = np.argmax(preds, axis = 1)\n  \n    #save results\n    loss.append(best_valid_loss)\n    acc.append(accuracy_score(test_y,preds))\n    auc.append(roc_auc_score(test_y,preds))\n    micro_F1.append(f1_score(test_y,preds,average='micro'))\n    macro_F1_weighted.append(f1_score(test_y,preds,average='weighted'))\n    binary_F1.append(f1_score(test_y,preds,average='binary'))\n    precision.append(precision_score(test_y,preds))\n    recall.append(recall_score(test_y,preds))\n    conf_matrix = confusion_matrix(test_y, preds)\n    conf_matrices.append(conf_matrix)","metadata":{"id":"krCHuXqsxn5l","execution":{"iopub.status.busy":"2022-01-16T14:16:53.318866Z","iopub.execute_input":"2022-01-16T14:16:53.319157Z","iopub.status.idle":"2022-01-16T14:16:53.331182Z","shell.execute_reply.started":"2022-01-16T14:16:53.319104Z","shell.execute_reply":"2022-01-16T14:16:53.330167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#implement cross validation + train/validate/predict\nloss = []\nacc = []\nauc = []\nmicro_F1 = []\nmacro_F1_weighted = []\nbinary_F1 = []\nprecision = []\nrecall = []\nconf_matrices = []\n\nfor fold, (train_index, test_index) in enumerate(kfold.split(df['text'], df['Label_bias_0-1'])):\n    sys.stdout.write('\\n \\r Fold {} / {}\\n'.format(fold+1,kfold.get_n_splits()))\n\n    #divide data into folds\n    train_text = df['text'].iloc[train_index]\n    test_text = df['text'].iloc[test_index]\n    train_labels = df['Label_bias_0-1'].iloc[train_index]\n    test_labels = df['Label_bias_0-1'].iloc[test_index]\n\n    #encode\n    train_encodings = tokenizer(train_text.tolist(), truncation=True, padding=True)\n    test_encodings = tokenizer(test_text.tolist(), truncation=True, padding=True)\n\n    #convert input to tensors \n    train_seq = torch.tensor(train_encodings['input_ids'])\n    train_mask = torch.tensor(train_encodings['attention_mask'])\n    train_y = torch.tensor(train_labels.tolist())\n\n    test_seq = torch.tensor(test_encodings['input_ids'])\n    test_mask = torch.tensor(test_encodings['attention_mask'])\n    test_y = torch.tensor(test_labels.tolist())\n\n    # wrap tensors into one dataset\n    train_data = TensorDataset(train_seq, train_mask, train_y)\n    test_data = TensorDataset(test_seq, test_mask, test_y)\n\n    #define dataloader\n    train_sampler = RandomSampler(train_data)\n    test_sampler = RandomSampler(test_data)\n    train_dataloader = DataLoader(train_data,sampler= train_sampler, batch_size=batch_size)\n    test_dataloader = DataLoader(test_data,sampler = test_sampler, batch_size=batch_size)\n\n    #create model instance with pre-trained weights and optimizer: insert respective model that is to be fine-tuned/evaluated\n#     model = BertClass()\n#     model = RobertaClass()\n#     model = BartClass()\n#     model = T5Class()\n    model.load_state_dict(weight_dict)\n    model.to(device)\n    optim_dbert = AdamW(model.parameters(), lr=1e-5)\n\n    #call train/validate/predict function\n    train_validate_pred(model)","metadata":{"id":"GhKHZ2S7Vxhx","outputId":"0d3218ce-c57f-437c-99c9-2b214e219f00","execution":{"iopub.status.busy":"2022-01-16T14:25:23.047768Z","iopub.execute_input":"2022-01-16T14:25:23.048044Z","iopub.status.idle":"2022-01-16T14:28:38.242361Z","shell.execute_reply.started":"2022-01-16T14:25:23.048015Z","shell.execute_reply":"2022-01-16T14:28:38.241642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#compute cross-validated performance metrics\ncv_loss = sum(loss)/len(loss)\ncv_acc = sum(acc)/len(acc)\ncv_auc = sum(auc)/len(auc)\ncv_micro_f1 = sum(micro_F1)/len(micro_F1)\ncv_macro_f1 = sum(macro_F1_weighted)/len(macro_F1_weighted)\nsd = np.std(macro_F1_weighted)\ncv_binary_f1 = sum(binary_F1)/len(binary_F1)\ncv_prec = sum(precision)/len(precision)\ncv_recall = sum(recall)/len(recall)\ncv_conf_matrix = np.mean(conf_matrices, axis=0)\n\n\nprint(\"CV Accuracy = {}\".format(round(cv_acc,4)))\nprint(\"CV AUC = {}\".format(round(cv_auc,4)))\nprint(\"CV Micro F1 = {}\".format(round(cv_micro_f1,4)))\nprint(\"CV Macro F1 weighted = {}\".format(round(cv_macro_f1,4)))\nprint(\"SD = {}\".format(round(sd,4)))\nprint(\"CV Binary F1 = {}\".format(round(cv_binary_f1,4)))\nprint(\"CV Precision = {}\".format(round(cv_prec,4)))\nprint(\"CV Recall = {}\".format(round(cv_recall,4)))\nprint(\"CV Loss = {}\".format(round(cv_loss,4)))","metadata":{"id":"2oVjkc1rwW85","execution":{"iopub.status.busy":"2022-01-16T14:28:38.243821Z","iopub.execute_input":"2022-01-16T14:28:38.244066Z","iopub.status.idle":"2022-01-16T14:28:38.256684Z","shell.execute_reply.started":"2022-01-16T14:28:38.244032Z","shell.execute_reply":"2022-01-16T14:28:38.254795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#optionally save metrics in dict\n#Roberta_DA_SG2_bs64_lr1e5_6ep = {\"loss\":cv_loss,\"micro_f1\":cv_micro_f1,\"macro_f1\":cv_macro_f1,\"SD\":sd,\"binary_f1\":cv_binary_f1,\"prec\":cv_prec,\"recall\":cv_recall}\n\n#store metrics in json format\n# with open('./Roberta_DA_SG2_bs64_lr1e5_6ep.json', 'w') as f:\n#     json.dump(Roberta_DA_SG2_bs64_lr1e5_6ep, f)","metadata":{"id":"ANOV_r5ZGWyV","execution":{"iopub.status.busy":"2022-01-08T09:47:10.407751Z","iopub.execute_input":"2022-01-08T09:47:10.408222Z","iopub.status.idle":"2022-01-08T09:47:10.416086Z","shell.execute_reply.started":"2022-01-08T09:47:10.408183Z","shell.execute_reply":"2022-01-08T09:47:10.415289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**McNemar test for statistical significance based on last cv split**","metadata":{"id":"bvJIY6pC-y0E"}},{"cell_type":"code","source":"from mlxtend.evaluate import mcnemar,mcnemar_table","metadata":{"id":"YzTY47jf-4Sb","execution":{"iopub.status.busy":"2022-01-16T14:15:36.632016Z","iopub.execute_input":"2022-01-16T14:15:36.632564Z","iopub.status.idle":"2022-01-16T14:15:36.685915Z","shell.execute_reply.started":"2022-01-16T14:15:36.632525Z","shell.execute_reply":"2022-01-16T14:15:36.685168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get predictions for model on test set. Insert the domain-adapted model you want to evaluate here. Predictions are provided in the repository and do not have to be computed separately\n# with torch.no_grad():\n#     preds_DA = model(test_seq.to(device), test_mask.to(device))\n#     preds_DA = preds_DA.detach().cpu().numpy()\n# preds_DA = np.argmax(preds_DA, axis = 1)\n\n#optionally store predictions\n#np.save(\"preds_T5_DA.npy\",preds_T5_DA)","metadata":{"id":"DTTUBpXxASAc","execution":{"iopub.status.busy":"2022-01-16T14:29:03.823608Z","iopub.execute_input":"2022-01-16T14:29:03.823872Z","iopub.status.idle":"2022-01-16T14:29:08.188384Z","shell.execute_reply.started":"2022-01-16T14:29:03.823843Z","shell.execute_reply":"2022-01-16T14:29:08.187624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #get predictions for baseline model. Insert the baseline model you want to evaluate here.\n# with torch.no_grad():\n#     preds_noDA = model(test_seq.to(device), test_mask.to(device))\n#     preds_noDA = preds_noDA.detach().cpu().numpy()\n# preds_noDA = np.argmax(preds_noDA, axis = 1)\n\n#optionally store predictions\n# np.save(\"preds_T5_noDA.npy\",preds_T5_noDA)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:34:34.190951Z","iopub.execute_input":"2022-01-14T11:34:34.191214Z","iopub.status.idle":"2022-01-14T11:34:36.39085Z","shell.execute_reply.started":"2022-01-14T11:34:34.191185Z","shell.execute_reply":"2022-01-14T11:34:36.390009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load predictions for baseline and domain-adapted model and get contingency table. Predictions are provided in the repository and do not have to be computed separately\npreds_noDA = np.load(\"preds_T5_noDA.npy\") #path might have to be adapted\npreds__DA = np.load(\"preds_T5_DA.npy\")\ntb = mcnemar_table(y_target=np.array(test_labels), \n                   y_model1=preds_roberta_noDA, \n                   y_model2=preds_roberta_DA)\n\nprint(tb)","metadata":{"id":"okzJKIcT-_Mh","outputId":"03f2037a-630a-4ae2-a3bf-dd1be7fe8b6d","execution":{"iopub.status.busy":"2022-01-16T14:31:44.127979Z","iopub.execute_input":"2022-01-16T14:31:44.128687Z","iopub.status.idle":"2022-01-16T14:31:44.145542Z","shell.execute_reply.started":"2022-01-16T14:31:44.128648Z","shell.execute_reply":"2022-01-16T14:31:44.144851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#calculate McNemar test statistic\nchi2, p = mcnemar(ary=tb, corrected=True)\nprint('chi-squared:', chi2)\nprint('p-value:', p)","metadata":{"id":"mSszecmS_gk-","outputId":"1b78c5b7-1370-4be4-bc1f-e131398ff254","execution":{"iopub.status.busy":"2022-01-16T14:31:46.998636Z","iopub.execute_input":"2022-01-16T14:31:46.999499Z","iopub.status.idle":"2022-01-16T14:31:47.006977Z","shell.execute_reply.started":"2022-01-16T14:31:46.999447Z","shell.execute_reply":"2022-01-16T14:31:47.006089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"V_s36EwTGkLJ"},"execution_count":null,"outputs":[]}]}