{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UN5U7W0b7X-b",
    "papermill": {
     "duration": 0.015104,
     "end_time": "2021-09-24T12:57:54.41903",
     "exception": false,
     "start_time": "2021-09-24T12:57:54.403926",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This script applies Domain-adaptive pretraining to BERT,RoBERTa,BART,and T5. The final pre-trained models can be found at: https://drive.google.com/drive/folders/1-A1hGKeu-27X9I4ySkja5vMlVscnF8GR?usp=sharing\n",
    "\n",
    "Required data to run this script:\n",
    "- the WNC corpus: https://github.com/rpryzant/neutralizing-bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EW98CVr7bMX",
    "outputId": "2b007272-ebe2-41e3-e7b6-17fefffdf702",
    "papermill": {
     "duration": 21.776564,
     "end_time": "2021-09-24T12:58:16.209532",
     "exception": false,
     "start_time": "2021-09-24T12:57:54.432968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install openpyxl\n",
    "!pip install sentencepiece\n",
    "import time\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import io\n",
    "import random\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score,f1_score,precision_score,recall_score,accuracy_score\n",
    "import transformers\n",
    "import sentencepiece\n",
    "from transformers import T5Tokenizer,T5EncoderModel,AdamW,BertModel,BertTokenizer,RobertaModel,RobertaTokenizer,BartModel,BartTokenizer\n",
    "from torch.utils.data import DataLoader,TensorDataset,ConcatDataset,RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMQo98GZfSlQ",
    "papermill": {
     "duration": 0.026304,
     "end_time": "2021-09-24T12:58:16.325607",
     "exception": false,
     "start_time": "2021-09-24T12:58:16.299303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function split train dataset into train, validation and test sets\n",
    "def train_test (text,labels,test_size):\n",
    "  train_text, test_text, train_labels, test_labels = train_test_split(text, \n",
    "                                                                    labels, \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=test_size,\n",
    "                                                                    stratify=labels)\n",
    "  return train_text, test_text, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kY5iqRlwfYd1",
    "papermill": {
     "duration": 7.772323,
     "end_time": "2021-09-24T12:58:24.116674",
     "exception": false,
     "start_time": "2021-09-24T12:58:16.344351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#function to tokenize sentences. Respective model must be uncommented\n",
    "#tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "#tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "#tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize(sentences,labels,max_length = None):\n",
    "  \"tokenizes input and returns tokenized input + labels as tensors\"\n",
    "\n",
    "  input_ids = []\n",
    "  attention_masks = []\n",
    "\n",
    "  for text in sentences.to_list():\n",
    "      encodings = tokenizer.encode_plus(text,add_special_tokens = True,max_length = max_length\n",
    "                                        ,truncation = True, padding = 'max_length',return_attention_mask = True)\n",
    "      input_ids.append(encodings['input_ids'])\n",
    "      attention_masks.append(encodings['attention_mask'])\n",
    "\n",
    "  return torch.tensor(input_ids),torch.tensor(attention_masks),torch.tensor(labels.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tYLxdLbofq1A",
    "papermill": {
     "duration": 0.030543,
     "end_time": "2021-09-24T12:58:24.171021",
     "exception": false,
     "start_time": "2021-09-24T12:58:24.140478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to get predictions for test data\n",
    "def predict(model,dataloader):\n",
    "\n",
    "  predictions = []\n",
    "  for batch in dataloader:\n",
    "    batch = [r.to(device) for r in batch]\n",
    "    sent_id, mask, labels = batch\n",
    "    with torch.no_grad():\n",
    "      output = model(sent_id, attention_mask=mask,labels = labels)\n",
    "      preds = output[1]\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "      predictions.append(np.argmax(preds, axis = 1).flatten())\n",
    "\n",
    "  #merge sublists of predictions\n",
    "  predictions = [label for batch in predictions for label in batch]\n",
    "\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8wGoA61M4gn",
    "papermill": {
     "duration": 0.03261,
     "end_time": "2021-09-24T12:58:24.225481",
     "exception": false,
     "start_time": "2021-09-24T12:58:24.192871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#set seed\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)   \n",
    "random.seed(0)    \n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kYHCogOqdfvC",
    "papermill": {
     "duration": 0.027235,
     "end_time": "2021-09-24T12:58:24.274741",
     "exception": false,
     "start_time": "2021-09-24T12:58:24.247506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#read WNC corpus \n",
    "df_wiki = pd.read_excel('WNC.xlsx')\n",
    "df_wiki.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xkb84DERme9r",
    "papermill": {
     "duration": 0.027531,
     "end_time": "2021-09-24T12:58:24.324983",
     "exception": false,
     "start_time": "2021-09-24T12:58:24.297452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train test split + tokenization\n",
    "train_text, test_text, train_labels, test_labels = train_test(df_wiki['text'], df_wiki['label_bias'],0.2)\n",
    "train_input_ids,train_attention_masks,train_y = tokenize(train_text, train_labels)\n",
    "test_input_ids,test_attention_masks,test_y = tokenize(test_text,test_labels) \n",
    "train_data_wiki = TensorDataset(train_input_ids, train_attention_masks, train_y)\n",
    "test_data_wiki = TensorDataset(test_input_ids, test_attention_masks, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3aj7vxNJTiM",
    "papermill": {
     "duration": 0.03044,
     "end_time": "2021-09-24T12:58:40.324228",
     "exception": false,
     "start_time": "2021-09-24T12:58:40.293788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define dataloader and epochs\n",
    "epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "train_sampler = RandomSampler(train_data_wiki)\n",
    "test_sampler = RandomSampler(test_data_wiki)\n",
    "\n",
    "train_dataloader = DataLoader(train_data_wiki,sampler= train_sampler, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data_wiki,sampler= test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_9S6NvBe-fO",
    "papermill": {
     "duration": 0.027027,
     "end_time": "2021-09-24T12:58:40.371438",
     "exception": false,
     "start_time": "2021-09-24T12:58:40.344411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define loss\n",
    "cross_entropy = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model:RoBERTa\n",
    "\n",
    "# class RobertaClass(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(RobertaClass, self).__init__()\n",
    "#         self.roberta = RobertaModel#.from_pretrained(\"roberta-base\")\n",
    "#         self.vocab_transform = torch.nn.Linear(768, 768)\n",
    "#         self.dropout = torch.nn.Dropout(0.2)\n",
    "#         self.classifier1 = nn.Linear(768,2)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask,labels):\n",
    "#         output_1 = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         hidden_state = output_1[0]\n",
    "#         pooler = hidden_state[:, 0]\n",
    "#         pooler = self.vocab_transform(pooler)\n",
    "#         pooler = self.dropout(pooler)\n",
    "#         output = self.classifier1(pooler)\n",
    "#         loss = cross_entropy(output,labels)\n",
    "\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9qI1SEdbHMt",
    "papermill": {
     "duration": 0.029042,
     "end_time": "2021-09-24T12:58:40.42673",
     "exception": false,
     "start_time": "2021-09-24T12:58:40.397688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create model: BART\n",
    "\n",
    "# class BartClass(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(BartClass, self).__init__()\n",
    "#         self.bart = BartModel.from_pretrained(\"facebook/bart-base\")\n",
    "#         self.vocab_transform = torch.nn.Linear(768, 768)\n",
    "#         self.dropout = torch.nn.Dropout(0.2)\n",
    "#         self.classifier1 = nn.Linear(768,2)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask,labels):\n",
    "#         output_1 = self.bart(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         hidden_state = output_1[0]\n",
    "#         pooler = hidden_state[:, 0]\n",
    "#         pooler = self.vocab_transform(pooler)\n",
    "#         pooler = self.dropout(pooler)\n",
    "#         output = self.classifier1(pooler)\n",
    "#         loss = cross_entropy(output,labels)\n",
    "\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model: Bert\n",
    "\n",
    "# class BertClass(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(BertClass, self).__init__()\n",
    "#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "#         self.vocab_transform = torch.nn.Linear(768, 768)\n",
    "#         self.dropout = torch.nn.Dropout(0.1)\n",
    "#         self.classifier1 = nn.Linear(768,2)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask,labels):\n",
    "#         output_1 = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         hidden_state = output_1[0]\n",
    "#         pooler = hidden_state[:, 0]\n",
    "#         pooler = self.vocab_transform(pooler)\n",
    "#         pooler = self.dropout(pooler)\n",
    "#         output = self.classifier1(pooler)\n",
    "#         loss = cross_entropy(output,labels)\n",
    "\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model: T5\n",
    "\n",
    "# class T5Class(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(T5Class, self).__init__()\n",
    "#         self.T5 = T5EncoderModel.from_pretrained(\"t5-base\")\n",
    "#         self.vocab_transform = torch.nn.Linear(768, 768)\n",
    "#         self.dropout = torch.nn.Dropout(0.1)\n",
    "#         self.classifier1 = nn.Linear(768,2)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask,labels):\n",
    "#         output_1 = self.T5(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         hidden_state = output_1[0]\n",
    "#         pooler = hidden_state[:, 0]\n",
    "#         pooler = self.vocab_transform(pooler)\n",
    "#         pooler = self.dropout(pooler)\n",
    "#         output = self.classifier1(pooler)\n",
    "#         loss = cross_entropy(output,labels)\n",
    "\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2BwHSj6cDri",
    "outputId": "aa93a2de-5060-47cc-c733-af3bcb442f12",
    "papermill": {
     "duration": 0.074651,
     "end_time": "2021-09-24T12:58:40.521086",
     "exception": false,
     "start_time": "2021-09-24T12:58:40.446435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#connect to GPU\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 29.992468,
     "end_time": "2021-09-24T12:59:10.534451",
     "exception": false,
     "start_time": "2021-09-24T12:58:40.541983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#instantiate model: uncomment model you want to train\n",
    "\n",
    "# model = BertClass()\n",
    "# model = RobertaClass()\n",
    "# model = BartClass()\n",
    "# model = T5Class()\n",
    "\n",
    "model = model.to(device)\n",
    "optim = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o79JiLrW5MfB",
    "papermill": {
     "duration": 0.031646,
     "end_time": "2021-09-24T12:59:10.587149",
     "exception": false,
     "start_time": "2021-09-24T12:59:10.555503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train function\n",
    "def train(dataloader):\n",
    "\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  counter = 0\n",
    "    \n",
    "  for index,batch in enumerate(dataloader):\n",
    "    counter += 1\n",
    "    sys.stdout.write('\\r Batch {}/{}'.format(counter,len(dataloader)))\n",
    "    optim.zero_grad()\n",
    "    batch = [r.to(device) for r in batch]\n",
    "    sent_id, mask, labels = batch\n",
    "    loss = model(sent_id, attention_mask=mask,labels = labels)\n",
    "    loss.backward() \n",
    "    total_loss = total_loss+loss.item()\n",
    "    optim.step()\n",
    "    del batch,sent_id,mask,labels\n",
    "        \n",
    "  avg_loss = total_loss / len(dataloader)\n",
    "  return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GYQcOX_fhjv",
    "papermill": {
     "duration": 0.028306,
     "end_time": "2021-09-24T12:59:10.637067",
     "exception": false,
     "start_time": "2021-09-24T12:59:10.608761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test function\n",
    "\n",
    "def validate(dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    print(\"\\nValidating...\")\n",
    "    counter = 0\n",
    "    for batch in dataloader:\n",
    "      counter +=1\n",
    "      batch = [r.to(device) for r in batch]\n",
    "      sent_id, mask, labels = batch\n",
    "\n",
    "      with torch.no_grad():\n",
    "        loss = model(sent_id, attention_mask=mask,labels = labels)\n",
    "        total_loss = total_loss+loss\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader) \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Est1Bn9UAb3w",
    "papermill": {
     "duration": 0.030173,
     "end_time": "2021-09-24T12:59:10.687414",
     "exception": false,
     "start_time": "2021-09-24T12:59:10.657241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train/validate function\n",
    "\n",
    "def train_validate(train_dataloader,test_dataloader):\n",
    "  best_valid_loss = float('inf')\n",
    "\n",
    "  # empty lists to store training and validation loss of each epoch\n",
    "  train_losses=[]\n",
    "  valid_losses=[]\n",
    "\n",
    "  #for each epoch\n",
    "  for epoch in range(epochs):\n",
    "        \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss = train(train_dataloader)\n",
    "    if torch.cuda.is_available():\n",
    "      torch.cuda.empty_cache()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss = validate(test_dataloader)\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "      best_valid_loss = valid_loss\n",
    "      torch.save(model.state_dict(), 'pytorch_model.bin') #insert path here\n",
    "      \n",
    "    #if validation loss increases, stop training\n",
    "    elif valid_loss >= best_valid_loss:\n",
    "      print(\"\\n Validation loss not decreased, Model of previous epoch saved\")\n",
    "      break\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTg22-3fbSI-",
    "outputId": "8398d94c-19ef-44c3-88d3-74c26987f02e",
    "papermill": {
     "duration": 0.025912,
     "end_time": "2021-09-24T12:59:10.733686",
     "exception": false,
     "start_time": "2021-09-24T12:59:10.707774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#apply training and validation\n",
    "train_validate(train_dataloader,test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
